{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 0:\n",
    "Recall that the logistic regression model's best performance on the test set was 93%. See what you can do to improve performance. Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires. Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge requires using supervised NLP to analyze a pre-labelled dataset for training and testing. \n",
    "\n",
    "The goal of this exercise (challenge) was to predict whether  a sentence comes from Alice in Wonderland by Lewis Carroll or Persuasion by Jane Austen. \n",
    "\n",
    "The following supervised models will be used in the challenge: Logistic regression, Linear SVM, Stocastich Gradient Descent, Gradient Boosting, Random Forest, and Navie Bayes. \n",
    "\n",
    "We use BoW - Bag of Words. To count how many times each word appears. We will then use those counts as features.\n",
    "\n",
    "We are using the F1-score metric and the training/testing accuracy scores to the determine the success of how well the supervised learning models perform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46284\n",
      "14139\n"
     ]
    }
   ],
   "source": [
    "print(len(persuasion))\n",
    "print(len(alice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting persuasion to same lenght as Alice to speed things up\n",
    "persuasion = persuasion[:141709]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Bag of Words \n",
    "\n",
    "To make a list of the most common 3000 words used. Counter at (if i % 700 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 3000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(3000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 700 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision</th>\n",
       "      <th>Clay</th>\n",
       "      <th>rouse</th>\n",
       "      <th>intervention</th>\n",
       "      <th>represent</th>\n",
       "      <th>breathe</th>\n",
       "      <th>Miss</th>\n",
       "      <th>sound</th>\n",
       "      <th>London</th>\n",
       "      <th>throwing</th>\n",
       "      <th>...</th>\n",
       "      <th>brilliancy</th>\n",
       "      <th>limited</th>\n",
       "      <th>man</th>\n",
       "      <th>boot</th>\n",
       "      <th>reality</th>\n",
       "      <th>prevent</th>\n",
       "      <th>submit</th>\n",
       "      <th>method</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  decision Clay rouse intervention represent breathe Miss sound London  \\\n",
       "0        0    0     0            0         0       0    0     0      0   \n",
       "1        0    0     0            0         0       0    0     0      0   \n",
       "2        0    0     0            0         0       0    0     0      0   \n",
       "3        0    0     0            0         0       0    0     0      0   \n",
       "4        0    0     0            0         0       0    0     0      0   \n",
       "\n",
       "  throwing     ...     brilliancy limited man boot reality prevent submit  \\\n",
       "0        0     ...              0       0   0    0       0       0      0   \n",
       "1        0     ...              0       0   0    0       0       0      0   \n",
       "2        0     ...              0       0   0    0       0       0      0   \n",
       "3        0     ...              0       0   0    0       0       0      0   \n",
       "4        0     ...              0       0   0    0       0       0      0   \n",
       "\n",
       "  method                                      text_sentence text_source  \n",
       "0      0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1      0  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2      0  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3      0                                      (Oh, dear, !)     Carroll  \n",
       "4      0                                      (Oh, dear, !)     Carroll  \n",
       "\n",
       "[5 rows x 1615 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "1. Logistic regression \n",
    "2. Naive Bayes\n",
    "3. Gradient Boosting\n",
    "4. Random Forest \n",
    "5. Linear SVM\n",
    "6. Stocastich Gradient Descent \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mille\\Conda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 1613) (266,)\n",
      "Training set score: 0.9699248120300752\n",
      "\n",
      "Test set score: 0.8764044943820225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.85      1.00      0.92       125\n",
      "     Carroll       1.00      0.58      0.74        53\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       178\n",
      "   macro avg       0.93      0.79      0.83       178\n",
      "weighted avg       0.89      0.88      0.87       178\n",
      "\n",
      "text_source  Austen  Carroll\n",
      "row_0                       \n",
      "Austen          106        5\n",
      "Carroll          19       48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "y_hat = lr.predict(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_hat))\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results from Logistic regression\n",
    "\n",
    "Test set score of 87.6% is pretty good!\n",
    "\n",
    "Books f1-scores \n",
    "\n",
    "Austen - 0.92: this is a great score means that the logistic regression was able to accurately determine the sentences to this author. \n",
    "\n",
    "Carroll - 0.74: this is not as strong of an f1-score. In the support used less sentences to fit the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.95      0.85      0.90       125\n",
      "     Carroll       0.72      0.91      0.80        53\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       178\n",
      "   macro avg       0.84      0.88      0.85       178\n",
      "weighted avg       0.88      0.87      0.87       178\n",
      "\n",
      "text_source  Austen  Carroll\n",
      "row_0                       \n",
      "Austen          106        5\n",
      "Carroll          19       48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "#Initialize and fit\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Apply to testing data\n",
    "y_hat = nb.predict(X_test)\n",
    "\n",
    "# Showing model performance\n",
    "cross = pd.crosstab(y_hat, y_test)\n",
    "print(\"Accuracy is: %0.3f\" % nb.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_hat))\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for Naive Bayes\n",
    "\n",
    "The test accuracy score of 86.5% is the same as the logistic regression model. \n",
    "\n",
    "Authors of Books f1-scores\n",
    "\n",
    "Austen - 0.90: slightly less then the logistic regression model. However it is still very high. \n",
    "\n",
    "Carroll - 0.80: showed improvement in modeling and predicting Carroll sentences from the logisitic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9661654135338346\n",
      "\n",
      "Test set score: 0.8202247191011236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.81      0.98      0.88       125\n",
      "     Carroll       0.89      0.45      0.60        53\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       178\n",
      "   macro avg       0.85      0.71      0.74       178\n",
      "weighted avg       0.83      0.82      0.80       178\n",
      "\n",
      "text_source  Austen  Carroll\n",
      "row_0                       \n",
      "Austen          106        5\n",
      "Carroll          19       48\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Apply to testing data\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "train = clf.fit(X_train, y_train)\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_hat))\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result of Gradient Boosting\n",
    "\n",
    "Test accuracy score: 82.02% which is less then the previous two models. \n",
    "\n",
    "F1-scores \n",
    "\n",
    "Austen - 0.88: this is a descent f1-score by itself, however it is slightly less then the previous two models. \n",
    "\n",
    "Carroll - 0.60: did a poor job at predicting the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mille\\Conda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9849624060150376\n",
      "\n",
      "Test set score: 0.8370786516853933\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.83      0.96      0.89       125\n",
      "     Carroll       0.85      0.55      0.67        53\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       178\n",
      "   macro avg       0.84      0.75      0.78       178\n",
      "weighted avg       0.84      0.84      0.83       178\n",
      "\n",
      "text_source  Austen  Carroll\n",
      "row_0                       \n",
      "Austen          106        5\n",
      "Carroll          19       48\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Apply to testing data\n",
    "y_hat = rfc.predict(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_hat))\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for Random forest\n",
    "\n",
    "Test accuracy: 83.7% which is roughly the same as the Gradient Boosting model. \n",
    "\n",
    "F1-scores:\n",
    "\n",
    "Austen - 0.89: is the same as the Gradient Boosting model. \n",
    "\n",
    "Carroll: 0.67: slight improvement from Gradient Boosting. However, this is not a very good score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.871\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.85      0.98      0.91       125\n",
      "     Carroll       0.94      0.60      0.74        53\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       178\n",
      "   macro avg       0.90      0.79      0.83       178\n",
      "weighted avg       0.88      0.87      0.86       178\n",
      "\n",
      "text_source  Austen  Carroll\n",
      "row_0                       \n",
      "Austen          123       21\n",
      "Carroll           2       32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "\n",
    "# Create instance and fit\n",
    "sv = SVC(kernel='linear')\n",
    "sv.fit(X_train, y_train)\n",
    "\n",
    "# Apply to testing data\n",
    "y_hat = sv.predict(X_test)\n",
    "\n",
    "# Showing model performance\n",
    "cross = pd.crosstab(y_hat, y_test)\n",
    "print(\"Accuracy is: %0.3f\" % sv.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_hat))\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Linear SVM \n",
    "\n",
    "Test accuracy: 87.1% the highest test accuracy of all the models. \n",
    "\n",
    "F1-scores: Are roughly identical to that of the logistic regression f1-scores at 0.91 (0.92 LR) for Austen and both at 0.74 for Carroll. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mille\\Conda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.87      0.91      0.89       125\n",
      "     Carroll       0.77      0.68      0.72        53\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       178\n",
      "   macro avg       0.82      0.80      0.81       178\n",
      "weighted avg       0.84      0.84      0.84       178\n",
      "\n",
      "text_source  Austen  Carroll\n",
      "row_0                       \n",
      "Austen          114       17\n",
      "Carroll          11       36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Create instance and fit\n",
    "sgdc = SGDClassifier(loss = 'hinge')\n",
    "sgdc.fit(X_train, y_train)\n",
    "\n",
    "# Apply to testing data\n",
    "y_hat = sgdc.predict(X_test)\n",
    "\n",
    "\n",
    "# Showing model performance\n",
    "cross = pd.crosstab(y_hat, y_test)\n",
    "print(\"Accuracy is: %0.3f\" % sgdc.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_hat))\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Stocastich Gradient Descent \n",
    "\n",
    "Test accuracy: 84.3% \n",
    "\n",
    "F1-scores: \n",
    "\n",
    "Austen - 0.89: this is a great score, however it is still not the best score from the previous models. \n",
    "\n",
    "Carroll - 0.72: a decent score but not the best score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall\n",
    "\n",
    "Comparing the f1-scores and test scores for all the models\n",
    "\n",
    "---------------| Logistic R.|----|Naive Bayes|----|Gradient Boosting|---|Random Forest|---|Linear SVM|---|Stocastic G.D.\n",
    "\n",
    "Austen -          **0.92**            0.90              0.88                 0.89               0.91          0.89\n",
    "\n",
    "Carroll -         **0.74**            0.80              0.60                 0.67               **0.74**       0.72\n",
    "\n",
    "Test score -      **87.6%**           86.5%             82.0%                83.7%              87.1%         84.3%\n",
    "\n",
    "\n",
    "\n",
    "The logistic regression model performed the best with NLP analysis. Logistic Regression had the highest f1-scores for both authors and highest testing accuracy. The Linear SVM model was very close to match the results of the logistic regression. Minor tweeks in the hyperparameters or in the model the Linear SVM model could produce a more favorable results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1:\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work. This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "\n",
    "Record your work for each challenge in a notebook and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma)\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "\n",
    "# Emma is quite long, let's cut it down to the same length as Alice.\n",
    "emma_sents = emma_sents[0:len(alice_sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Logistic Regression with Linear SVM in the new model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.6926829268292682\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.69      0.95      0.79       129\n",
      "     Carroll       0.74      0.26      0.39        76\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       205\n",
      "   macro avg       0.71      0.60      0.59       205\n",
      "weighted avg       0.71      0.69      0.64       205\n",
      "\n",
      "text_source  Austen  Carroll\n",
      "row_0                       \n",
      "Austen          114       17\n",
      "Carroll          11       36\n"
     ]
    }
   ],
   "source": [
    "# Now we can model it!\n",
    "# Let's use logistic regression again.\n",
    "\n",
    "# Combine the Emma sentence data with the Alice data from the test set.\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)\n",
    "print(classification_report(y_Emma_test, lr_Emma_predicted))\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.6048780487804878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.65      0.81      0.72       129\n",
      "     Carroll       0.44      0.26      0.33        76\n",
      "\n",
      "   micro avg       0.60      0.60      0.60       205\n",
      "   macro avg       0.55      0.53      0.53       205\n",
      "weighted avg       0.57      0.60      0.58       205\n",
      "\n",
      "text_source  Austen  Carroll\n",
      "row_0                       \n",
      "Austen          114       17\n",
      "Carroll          11       36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# Create instance and fit\n",
    "sv = SVC(kernel='linear')\n",
    "sv.fit(X_train, y_train)\n",
    "\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', sv.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = sv.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)\n",
    "print(classification_report(y_Emma_test, lr_Emma_predicted))\n",
    "print(cross)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Logistic Regression  69.2% test accuracy\n",
    "f1-scores \n",
    "\n",
    "Austen - 0.79\n",
    "\n",
    "Carrol - 0.39\n",
    "\n",
    "\n",
    "Linear SVM - 60.4% test accuracy \n",
    "\n",
    "Austen - 0.72\n",
    "\n",
    "Carroll - 0.33 \n",
    "\n",
    "The logistic regression model out performed the Linear SVM model. All the metric we used to determine the a more successful model were observed in the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
